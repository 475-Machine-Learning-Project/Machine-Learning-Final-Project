{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/damianomarsili/temp/blob/main/Final_Project_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9VDfC-ie19P"
      },
      "source": [
        "# Image Inpainting with GAN's\n",
        "Project mentor: Guanghui Qin\n",
        "\n",
        "Kevin Kim <kkim170@jh.edu>, Camden Shultz <cshultz3@jh.edu>, Jocelyn Hsu <jhsu37@jh.edu>, Damiano Marsili <dmarsil1@jh.edu>\n",
        "\n",
        "LINK TO GHUB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqwI3PT-hBJo"
      },
      "source": [
        "# Outline and Deliverables - TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7Af6y48e7HI"
      },
      "source": [
        "TODO: LINKS TO SECTIONS\n",
        "\n",
        "### Uncompleted Deliverables\n",
        "1. Different input images: We were unable to extend our use-case past landscape images. Our model will work for different types of images, but the accuracy is poor on non-landscape images. Extending our use-case would have required a significantly larger training set and thus a greatly increased training time. We did not have the time or computational resources to do so. However, we are confident given enough training data and time, our implementation can be extended to different image domains.\n",
        "\n",
        "\n",
        "### Completed Deliverables\n",
        "1. Masks on images: We discuss our masking and preprocessing [in \"Pre-processing\" below](##Pre-processing).\n",
        "2. Trained a GAN on landscape images: We discuss training our GAN [in \"Methods\" below](TODO).\n",
        "3. Successfully inpainted images: We discuss our inpainting technique [in \"?\" below](TODO).\n",
        "4. Implemented Navier-Stokes baseline comparison: We discuss our baselines [in \"Baselines\" below](TODO).\n",
        "5. Implemented FID evaluation metric: We discuss our evaluation metric [in \"Experimental Setup\" below](TODO).\n",
        "\n",
        "\n",
        "### Additional Deliverables\n",
        "1. We decided to add a Fast-Marching algorithm as a second baseline metric. We discuss this [in \"Baselines\" below](TODO).\n",
        "2. We experimented with different mask sizes and were able to maintain decent accuracy. We discuss this [in \"?\" below](TODO).\n",
        "3. We added a feature allowing users to upload their own images and see how our model performs on them. We discuss this [in \"?\" below](TODO)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eiq2aSauhSsS"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Definition\n",
        "\n",
        "Our final project tackles the task of image inpainting, which refers to the challenge of repairing missing or damaged portions of an image in such a way that the generated image closely resembles the original image. As an example, in the images below, an inpainting solution would attempt to fill in the white squares to best reproduce an input that can plausibly pass for the original image. To achieve this task, we will use a Generative Adversarial Network (GAN).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Example of inpainting challenge](./sample_imgs/intro1.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use cases\n",
        "Image inpainting is a widely applicable image editing technique. One of the most useful applications is repairing damaged or old images. An example of this is given in the images below. The input images on the left have visible signs of damage, and we can use inpainting to 'repair' the images, as shown with the images on the right.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Use cases](./sample_imgs/use_case.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another application of image inpainting is the ability to remove unwanted items from an image. For instance, a picture of a scenic landscape taken on vacation may be interrupted by a tourist. Image inpainting can be used to remove the tourist from the image and restore the scenic landscape."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Uniqueness\n",
        "Image inpainting and the broader topic of GANs is unique and exciting as we are creating meaningful new data without labels. In contrast to the majority of the algorithms we covered in the course, which revolve around classification or regression, we are instead using machine learning to produce new data that can plausibly pass for a sample instance in the input dataset.\n",
        "\n",
        "Our approach with GANs is related to our learning of autoencoders in class. We draw this comparison as we are attempting to learn properties of the input in an attempt to regenerate instances similar to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ethical Implications\n",
        "There is a significant ethical implication of the task, and that is that it can be used for deception if it is used incorrectly. For instance, image inpainting can be used to remove evidence from pictures of a crime, or produce realistic false images for the purpose of fake news. Some examples of falsified images are:\n",
        "* Time's cover with OJ's skin darkened (June 27, 1994)\n",
        "* National Geographic's cover of the pyramids\n",
        "* Ford's advertisement in which a black person was changed into a white person\n",
        "* A photo of Bill Clinton together with Ronald Reagan... published before the two ever met"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n",
        "For our project we utilized the MIT Places dataset, Which is one of the largest image datasets. We chose the MIT Places Dataset as it provides high coverage and high diversity of examples for landscapes, which we identified as our target image set. The dataset features 10 million 256x256 images covering 434 disjoint categories of scenes and landscapes. For primary testing, we will mainly use low variance images, as we hypothesize these will be easier to inpaint due to the low variance of RGB values for all the pixels in the image. Our low variance set includes the \"Snow field\", \"Sky\", \"Mountains\" and \"Cornfields\" image set. Each of the four sets contains around 15,000 images, for a total set of 60,000 images. We show a handful of examples below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lOicoBYif7g"
      },
      "outputs": [],
      "source": [
        "# TODO: Load your data and print 2-3 examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN1fYEfGidiD"
      },
      "source": [
        "## Pre-processing\n",
        "\n",
        "TODO: this section\n",
        "\n",
        "What features did you use or choose not to use? Why?\n",
        "\n",
        "If you have categorical labels, were your datasets class-balanced?\n",
        "\n",
        "How did you deal with missing data? What about outliers?\n",
        "\n",
        "What approach(es) did you use to pre-process your data? Why?\n",
        "\n",
        "Are your features continuous or categorical? How do you treat these features differently?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEuKEzM5ipag"
      },
      "outputs": [],
      "source": [
        "# For those same examples above, what do they look like after being pre-processed?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cDLEwhAx0gP"
      },
      "outputs": [],
      "source": [
        "# Visualize the distribution of your data before and after pre-processing.\n",
        "#   You may borrow from how we visualized data in the Lab homeworks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tASjmmtjiwvu"
      },
      "source": [
        "# Models and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlrwR9E1hnQ3"
      },
      "source": [
        "## Experimental Setup\n",
        "As our evaluation metric, we decided to use Frechlet Inception Distance (FID). We had originally planned to use Inception Score (IS), but found that FID was more sensitive to small quantitative changes in the image that are extremely noticeable by humans. This was particularly true of blurring, where IS scored blurry images much lower than FID (where lower scores indicate a better generated image). FID works by comparing the activations of a deep layer of a pretrained model named Inception v3. Since we are considering the activations of a deep layer close to the output, the activations effectively capture similarity as a human would interpret it. The formula for FID is given as:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$FID = \t\\lVert  \\mu - \\mu_w  \\rVert^2_2 + tr(\\Sigma + \\Sigma_w - 2(\\Sigma^{1/2} \\Sigma_w \\Sigma^{1/2})^{1/2})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Where $\\mu$ and $\\mu_w$ are the means and $\\Sigma$ and $\\Sigma_w$ are the covariance matrices of the activation scores of the two images passed to the Inception v3 model. We implement FID as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import sqrtm\n",
        "\n",
        "def fid(inception_model, imgs1, imgs2):\n",
        "  # Extract convolutions from inception model\n",
        "  conv1 = inception_model.predict(imgs1)\n",
        "  conv2 = inception_model.predict(imgs2)\n",
        "\n",
        "  # Compute mean & cov\n",
        "  m1, cov1 = conv1.mean(axis=0), np.cov(conv1, rowvar=False)\n",
        "  m2, cov2 = conv2.mean(axis=0), np.cov(conv2, rowvar=False)\n",
        "\n",
        "  # Sum squared difference of means\n",
        "  ss = np.sum((m1 - m2) ** 2)\n",
        "  \n",
        "  # Square root of cov product\n",
        "  mean_cov = sqrtm(cov1.dot(cov2))\n",
        "\n",
        "  # Sanity check for im numbers\n",
        "  if np.iscomplexobj(mean_cov):\n",
        "    mean_cov = mean_cov.real\n",
        "  \n",
        "  FID = ss + np.trace(cov1 + cov2 - 2.0 * mean_cov)\n",
        "  return FID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For our loss function, we opted for (? TODO). We chose this loss function as (? TODO). We also attempted to use (? TODO: alternative loss) loss, although we found this function to perform worse upon experimentation. One reason for this may be (? TODO). The formula for our loss function is given as:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TODO: loss function formula"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUNxC358jPDr"
      },
      "outputs": [],
      "source": [
        "# TODO: Code for loss functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For our data split, we decided on a (TODO: ?) split. We decided on this split as we felt we needed as many images as possible for training, since the model would need to learn a fairly complex representation in order to generate plausible images. Since testing relies on a pretrained GAN, we felt we did not need an excessively large testing set to evaluate our models performance appropriately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMyqHUa0jUw7"
      },
      "source": [
        "## Baselines \n",
        "We compared our model against two analytical baselines: Navier-Stokes and Fast Marching. Navier-Stokes is a differential equations based solution which works by projecting eigenvectors into the masked region and using those to predict the missing pixel values. On the other hand, Fast Marching works by evaluating a neighborhood of pixels around the border of the mask and using those to determine the values of the missing pixels, slowly working towards the center. These are reasonable baselines as they are standard pre-machine-learning methods for image inpainting, and therefore are extremely well documented and their effectiveness has been previously established. Moreover, these baseline methods are extremely easy to implement, so we could focus our attention on our model. We show some sample inpainted pictures using the baseline methods below. The image on the left shows the original image, the middle image shows the mask and the rightmost image shows the analytical inpainted solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Navier Stokes:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Navier Stokes example](./sample_imgs/nav_stokes1.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fast-Marching:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Fast-Marching image](./sample_imgs/fast_marching1.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqB48IF9kMBf"
      },
      "source": [
        "## Methods\n",
        "\n",
        "We chose to implement the Generative Adversarial Network (GAN) for our image inpainting task. We chose this method because our ultimate goal is to infill a missing portion of an image by building a generator, and the adversarial nature of GAN with the addition of a discriminator allows us to improve our generator's performance. First, we trained the discriminator on real landscape data and (untrained) generator-created data. Afterwards, we let the generator inpaint the masked portion of images and optimize the generator's performance by running the discriminator on the generator's output. \n",
        "\n",
        "Once the generator was trained and hyperparameters tuned in accordance with the minimax loss (minimize with respect to the discriminator and maximize with respect to the generator), we completed the image inpainting task on our test set. We evaluated the model's performance on the test set with the Frechlet Inception Distance (FID) due to its sensitivity to small pixel value changes, which is essential to achieving realistic image inpainting.\n",
        "\n",
        "Overall, it was easy to implement (TODO: ). We found training the GAN to be challenging, as the disciminator loss often decreased to 0 due to vanishing gradients, causing the generator to inpaint masks with very incorrect values. Furthermore, GAN requires a very large dataset to train, so our model's performance was limited by computational resources. (TODO: still adding Poisson blending?)\n",
        "\n",
        "(TODO) For each method, what hyperparameters did you evaluate? How sensitive was your model's performance to different hyperparameter settings?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ma4JoDzar6xU"
      },
      "outputs": [],
      "source": [
        "# Code for training models, or link to your Git repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO_kP1fmkWWk"
      },
      "outputs": [],
      "source": [
        "# Show plots of how these models performed during training.\n",
        "#  For example, plot train loss and train accuracy (or other evaluation metric) on the y-axis,\n",
        "#  with number of iterations or number of examples on the x-axis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zdp4_H-kx8H"
      },
      "source": [
        "## Results\n",
        "\n",
        "Show tables comparing your methods to the baselines.\n",
        "\n",
        "What about these results surprised you? Why?\n",
        "\n",
        "Did your models over- or under-fit? How can you tell? What did you do to address these issues?\n",
        "\n",
        "What does the evaluation of your trained models tell you about your data? How do you expect these models might behave differently on different data?  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS2sjfbglG_V"
      },
      "outputs": [],
      "source": [
        "# Show plots or visualizations of your evaluation metric(s) on the train and test sets.\n",
        "#   What do these plots show about over- or under-fitting?\n",
        "#   You may borrow from how we visualized results in the Lab homeworks.\n",
        "#   Are there aspects of your results that are difficult to visualize? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59EbS1GilSQ_"
      },
      "source": [
        "# Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugJXhZKNlUT4"
      },
      "source": [
        "## What you've learned\n",
        "\n",
        "Convolutional neural networks as well as loss functions learned in class were most relevant to our project, as our GAN model is a specific type of neural network, and we needed to consider the best loss functions for our image inpainting task. What we found most surprising was the ability for computers to simulate creativity. (TODO: check if we're still doing poisson blending) If we had two more weeks to work on our project, we'd like to incorporate Poisson blending to achieve seamless edges between the infilled mask and the original image.\n",
        "\n",
        "\n",
        "What lessons did you take from this project that you want to remember for the next ML project you work on? Do you think those lessons would transfer to other datasets and/or models? Why or why not?\n",
        "\n",
        "What was the most helpful feedback you received during your presentation? Why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Final Project Submission Template",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
